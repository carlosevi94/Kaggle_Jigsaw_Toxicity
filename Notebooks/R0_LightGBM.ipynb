{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot, hashing_trick, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import pipeline\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGS:\n",
    "    import logging\n",
    "\n",
    "    if not os.path.exists('../logs/'):\n",
    "        os.makedirs('../logs/')\n",
    "\n",
    "    NAME = 'Exploracion'\n",
    "\n",
    "    LOG_NAME = '../logs/{}_{}.log'.format(datetime.datetime.now().strftime(\"%Y%m%d\"), NAME)\n",
    "    logging.basicConfig(filename=LOG_NAME, level=logging.WARNING, format='%(asctime)s %(message)s')\n",
    "\n",
    "    logging.warning(\"\")\n",
    "    logging.warning(\"Comienzo script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../data/train.csv', usecols=['comment_text', 'target'], nrows=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = dd.read_csv('../data/train.csv', usecols=['comment_text', 'target']).head(50000)\n",
    "train = dd.read_csv('../data/train.csv').head(250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139857, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59859</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59861</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>hahahahahahahahhha suck it.</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>239575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The ranchers seem motivated by mostly by greed...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26662</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>It was a great show. Not a combo I'd of expect...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26650</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    target                                       comment_text  \\\n",
       "0   59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1   59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2   59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3   59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4   59856  0.893617               haha you guys are a bunch of losers.   \n",
       "5   59859  0.666667                               ur a sh*tty comment.   \n",
       "6   59861  0.457627                        hahahahahahahahhha suck it.   \n",
       "7   59863  0.000000                                FFFFUUUUUUUUUUUUUUU   \n",
       "8  239575  0.000000  The ranchers seem motivated by mostly by greed...   \n",
       "9  239576  0.000000  It was a great show. Not a combo I'd of expect...   \n",
       "\n",
       "   severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "0         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "2         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "3         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "4         0.021277  0.000000         0.021277  0.872340     0.0    0.0   \n",
       "5         0.047619  0.638095         0.000000  0.333333     0.0    NaN   \n",
       "6         0.050847  0.305085         0.000000  0.254237     0.0    NaN   \n",
       "7         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "8         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "9         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "\n",
       "   atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "1      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "2      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "3      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "4      0.0  ...        2006  rejected      0    0    0      1         0   \n",
       "5      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "6      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "7      NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "8      NaN  ...       26662  approved      0    0    0      0         0   \n",
       "9      NaN  ...       26650  approved      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0         0.000000                         0                         4  \n",
       "1         0.000000                         0                         4  \n",
       "2         0.000000                         0                         4  \n",
       "3         0.000000                         0                         4  \n",
       "4         0.000000                         4                        47  \n",
       "5         0.009524                         0                       105  \n",
       "6         0.220339                         0                        59  \n",
       "7         0.000000                         0                         4  \n",
       "8         0.000000                         0                         4  \n",
       "9         0.000000                         0                         4  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "5                                 ur a sh*tty comment.\n",
       "6                          hahahahahahahahhha suck it.\n",
       "7                                  FFFFUUUUUUUUUUUUUUU\n",
       "8    The ranchers seem motivated by mostly by greed...\n",
       "9    It was a great show. Not a combo I'd of expect...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = train['comment_text']\n",
    "text_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "weights = np.ones((len(text_data),)) / 4\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "# weights = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25, 0.5 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrBad = [\n",
    "'acrotomophilia',\n",
    "'anal',\n",
    "'anilingus',\n",
    "'anus',\n",
    "'arsehole',\n",
    "'ass',\n",
    "'asshole',\n",
    "'assmunch',\n",
    "'autoerotic',\n",
    "'babeland',\n",
    "'bangbros',\n",
    "'bareback',\n",
    "'barenaked',\n",
    "'bastardo',\n",
    "'bastinado',\n",
    "'bbw',\n",
    "'bdsm',\n",
    "'bestiality',\n",
    "'bimbos',\n",
    "'birdlock',\n",
    "'bitch',\n",
    "'blumpkin',\n",
    "'bollocks',\n",
    "'bondage',\n",
    "'boner',\n",
    "'boob',\n",
    "'boobs',\n",
    "'bukkake',\n",
    "'bulldyke',\n",
    "'bullet vibe',\n",
    "'bung hole',\n",
    "'bunghole',\n",
    "'busty',\n",
    "'butt',\n",
    "'buttcheeks',\n",
    "'butthole',\n",
    "'camgirl',\n",
    "'camslut',\n",
    "'camwhore',\n",
    "'carpetmuncher',\n",
    "'circlejerk',\n",
    "'clit',\n",
    "'clitoris',\n",
    "'clusterfuck',\n",
    "'cock',\n",
    "'cocks',\n",
    "'coprolagnia',\n",
    "'coprophilia',\n",
    "'cornhole',\n",
    "'cum',\n",
    "'cumming',\n",
    "'cunnilingus',\n",
    "'cunt',\n",
    "'darkie',\n",
    "'daterape',\n",
    "'deepthroat',\n",
    "'dick',\n",
    "'dildo',\n",
    "'doggiestyle',\n",
    "'doggystyle',\n",
    "'dolcett',\n",
    "'domination',\n",
    "'dominatrix',\n",
    "'dommes',\n",
    "'ecchi',\n",
    "'ejaculation',\n",
    "'erotic',\n",
    "'erotism',\n",
    "'escort',\n",
    "'eunuch',\n",
    "'faggot',\n",
    "'fecal',\n",
    "'felch',\n",
    "'fellatio',\n",
    "'feltch',\n",
    "'femdom',\n",
    "'figging',\n",
    "'fingering',\n",
    "'fisting',\n",
    "'footjob',\n",
    "'frotting',\n",
    "'fuck',\n",
    "'fucking',\n",
    "'fudgepacker',\n",
    "'futanari',\n",
    "'gay',\n",
    "'genitals',\n",
    "'goatcx',\n",
    "'goatse',\n",
    "'gokkun',\n",
    "'goodpoop',\n",
    "'goregasm',\n",
    "'grope',\n",
    "'guro',\n",
    "'handjob',\n",
    "'hardcore',\n",
    "'hentai',\n",
    "'homoerotic',\n",
    "'honkey',\n",
    "'hooker',\n",
    "'kill',\n",
    "'murder',\n",
    "'fat',\n",
    "'humping',\n",
    "'incest',\n",
    "'intercourse',\n",
    "'jack',\n",
    "'jerk',\n",
    "'jigaboo',\n",
    "'jiggaboo',\n",
    "'jiggerboo',\n",
    "'jizz',\n",
    "'juggs',\n",
    "'kike',\n",
    "'kinbaku',\n",
    "'kinkster',\n",
    "'kinky',\n",
    "'knobbing',\n",
    "'lolita',\n",
    "'lovemaking',\n",
    "'masturbate',\n",
    "'motherfucker',\n",
    "'muffdiving',\n",
    "'nambla',\n",
    "'nawashi',\n",
    "'negro',\n",
    "'neonazi',\n",
    "'nigga',\n",
    "'nigger',\n",
    "'nimphomania',\n",
    "'nipple',\n",
    "'nipples',\n",
    "'nude',\n",
    "'nudity',\n",
    "'nympho',\n",
    "'nymphomania',\n",
    "'octopussy',\n",
    "'omorashi',\n",
    "'orgasm',\n",
    "'orgy',\n",
    "'paedophile',\n",
    "'panties',\n",
    "'panty',\n",
    "'pedobear',\n",
    "'pedophile',\n",
    "'pegging',\n",
    "'penis',\n",
    "'pissing',\n",
    "'pisspig',\n",
    "'playboy',\n",
    "'ponyplay',\n",
    "'poof',\n",
    "'poopchute',\n",
    "'porn',\n",
    "'porno',\n",
    "'pornography',\n",
    "'pthc',\n",
    "'pubes',\n",
    "'pussy',\n",
    "'queaf',\n",
    "'raghead',\n",
    "'rape',\n",
    "'raping',\n",
    "'rapist',\n",
    "'rectum',\n",
    "'cowgirl',\n",
    "'rimjob',\n",
    "'rimming',\n",
    "'sadism',\n",
    "'scat',\n",
    "'schlong',\n",
    "'scissoring',\n",
    "'semen',\n",
    "'sex',\n",
    "'sexo',\n",
    "'sexy',\n",
    "'beaver',\n",
    "'pussy',\n",
    "'shemale',\n",
    "'shibari',\n",
    "'shit',\n",
    "'shota',\n",
    "'shrimping',\n",
    "'slanteye',\n",
    "'slut',\n",
    "'smut',\n",
    "'snatch',\n",
    "'snowballing',\n",
    "'sodomize',\n",
    "'sodomy',\n",
    "'spic',\n",
    "'spooge',\n",
    "'strapon',\n",
    "'strappado',\n",
    "'strip',\n",
    "'suck',\n",
    "'sucks',\n",
    "'suicide',\n",
    "'sultry',\n",
    "'swastika',\n",
    "'swinger',\n",
    "'threesome',\n",
    "'throating',\n",
    "'tit',\n",
    "'tits',\n",
    "'titties',\n",
    "'titty',\n",
    "'topless',\n",
    "'tosser',\n",
    "'towelhead',\n",
    "'tranny',\n",
    "'tribadism',\n",
    "'tubgirl',\n",
    "'tushy',\n",
    "'twat',\n",
    "'twink',\n",
    "'twinkie',\n",
    "'undressing',\n",
    "'upskirt',\n",
    "'urophilia',\n",
    "'vagina',\n",
    "'vibrator',\n",
    "'vorarephilia',\n",
    "'voyeur',\n",
    "'vulva',\n",
    "'wank',\n",
    "'wetback',\n",
    "'xx',\n",
    "'xxx',\n",
    "'yaoi',\n",
    "'yiffy',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrGood = ['absolutely',\n",
    " 'abundant',\n",
    " 'accept',\n",
    " 'acclaimed',\n",
    " 'accomplishment',\n",
    " 'achievement',\n",
    " 'action',\n",
    " 'active',\n",
    " 'activist',\n",
    " 'acumen',\n",
    " 'adjust',\n",
    " 'admire',\n",
    " 'adopt',\n",
    " 'adorable',\n",
    " 'adored',\n",
    " 'adventure',\n",
    " 'affirmation',\n",
    " 'affirmative',\n",
    " 'affluent',\n",
    " 'agree',\n",
    " 'airy',\n",
    " 'alive',\n",
    " 'alliance',\n",
    " 'ally',\n",
    " 'alter',\n",
    " 'amaze',\n",
    " 'amity',\n",
    " 'animated',\n",
    " 'answer',\n",
    " 'appreciation',\n",
    " 'approve',\n",
    " 'aptitude',\n",
    " 'artistic',\n",
    " 'assertive',\n",
    " 'astonish',\n",
    " 'astounding',\n",
    " 'astute',\n",
    " 'attractive',\n",
    " 'authentic',\n",
    " 'basic',\n",
    " 'beaming',\n",
    " 'beautiful',\n",
    " 'believe',\n",
    " 'benefactor',\n",
    " 'benefit',\n",
    " 'bighearted',\n",
    " 'blessed',\n",
    " 'bliss',\n",
    " 'bloom',\n",
    " 'bountiful',\n",
    " 'bounty',\n",
    " 'brave',\n",
    " 'bright',\n",
    " 'brilliant',\n",
    " 'bubbly',\n",
    " 'bunch',\n",
    " 'burgeon',\n",
    " 'calm',\n",
    " 'care',\n",
    " 'celebrate',\n",
    " 'certain',\n",
    " 'change',\n",
    " 'character',\n",
    " 'charitable',\n",
    " 'charming',\n",
    " 'cheer',\n",
    " 'cherish',\n",
    " 'clarity',\n",
    " 'classy',\n",
    " 'clean',\n",
    " 'clever',\n",
    " 'closeness',\n",
    " 'commend',\n",
    " 'companionship',\n",
    " 'complete',\n",
    " 'comradeship',\n",
    " 'confident',\n",
    " 'connect',\n",
    " 'connected',\n",
    " 'constant',\n",
    " 'content',\n",
    " 'conviction',\n",
    " 'copious',\n",
    " 'core',\n",
    " 'coupled',\n",
    " 'courageous',\n",
    " 'creative',\n",
    " 'cuddle',\n",
    " 'cultivate',\n",
    " 'cure',\n",
    " 'curious',\n",
    " 'cute',\n",
    " 'dazzling',\n",
    " 'delight',\n",
    " 'direct',\n",
    " 'discover',\n",
    " 'distinguished',\n",
    " 'divine',\n",
    " 'donate',\n",
    " 'each day',\n",
    " 'eager',\n",
    " 'earnest',\n",
    " 'easy',\n",
    " 'ecstasy',\n",
    " 'effervescent',\n",
    " 'efficient',\n",
    " 'effortless',\n",
    " 'electrifying',\n",
    " 'elegance',\n",
    " 'embrace',\n",
    " 'encompassing',\n",
    " 'encourage',\n",
    " 'endorse',\n",
    " 'energized',\n",
    " 'energy',\n",
    " 'enjoy',\n",
    " 'enormous',\n",
    " 'enthuse',\n",
    " 'enthusiastic',\n",
    " 'entirely',\n",
    " 'essence',\n",
    " 'established',\n",
    " 'esteem',\n",
    " 'everyday',\n",
    " 'everyone',\n",
    " 'excited',\n",
    " 'exciting',\n",
    " 'exhilarating',\n",
    " 'expand',\n",
    " 'explore',\n",
    " 'express',\n",
    " 'exquisite',\n",
    " 'exultant',\n",
    " 'faith',\n",
    " 'familiar',\n",
    " 'family',\n",
    " 'famous',\n",
    " 'feat',\n",
    " 'fit',\n",
    " 'flourish',\n",
    " 'fortunate',\n",
    " 'fortune',\n",
    " 'freedom',\n",
    " 'fresh',\n",
    " 'friendship',\n",
    " 'full',\n",
    " 'funny',\n",
    " 'gather',\n",
    " 'generous',\n",
    " 'genius',\n",
    " 'genuine',\n",
    " 'give',\n",
    " 'glad',\n",
    " 'glow',\n",
    " 'good',\n",
    " 'gorgeous',\n",
    " 'grace',\n",
    " 'graceful',\n",
    " 'gratitude',\n",
    " 'green',\n",
    " 'grin',\n",
    " 'group',\n",
    " 'grow',\n",
    " 'handsome',\n",
    " 'happy',\n",
    " 'harmony',\n",
    " 'healed',\n",
    " 'healing',\n",
    " 'healthful',\n",
    " 'healthy',\n",
    " 'heart',\n",
    " 'hearty',\n",
    " 'heavenly',\n",
    " 'helpful',\n",
    " 'here',\n",
    " 'hold',\n",
    " 'holy',\n",
    " 'honest',\n",
    " 'honor',\n",
    " 'hug',\n",
    " 'idea',\n",
    " 'ideal',\n",
    " 'imaginative',\n",
    " 'increase',\n",
    " 'incredible',\n",
    " 'independent',\n",
    " 'ingenious',\n",
    " 'innate',\n",
    " 'innovate',\n",
    " 'inspire',\n",
    " 'instantaneous',\n",
    " 'instinct',\n",
    " 'intellectual',\n",
    " 'intelligence',\n",
    " 'intuitive',\n",
    " 'inventive',\n",
    " 'joined',\n",
    " 'jovial',\n",
    " 'joy',\n",
    " 'jubilation',\n",
    " 'keen',\n",
    " 'key',\n",
    " 'kind',\n",
    " 'kiss',\n",
    " 'knowledge',\n",
    " 'laugh',\n",
    " 'leader',\n",
    " 'learn',\n",
    " 'legendary',\n",
    " 'light',\n",
    " 'lively',\n",
    " 'love',\n",
    " 'loveliness',\n",
    " 'lucidity',\n",
    " 'lucrative',\n",
    " 'luminous',\n",
    " 'maintain',\n",
    " 'marvelous',\n",
    " 'master',\n",
    " 'meaningful',\n",
    " 'meditate',\n",
    " 'mend',\n",
    " 'metamorphosis',\n",
    " 'mind-blowing',\n",
    " 'miracle',\n",
    " 'mission',\n",
    " 'modify',\n",
    " 'motivate',\n",
    " 'moving',\n",
    " 'natural',\n",
    " 'nature',\n",
    " 'nourish',\n",
    " 'nourished',\n",
    " 'novel',\n",
    " 'now',\n",
    " 'nurture',\n",
    " 'nutritious',\n",
    " 'one',\n",
    " 'open',\n",
    " 'openhanded',\n",
    " 'optimistic',\n",
    " 'paradise',\n",
    " 'party',\n",
    " 'peace',\n",
    " 'perfect',\n",
    " 'phenomenon',\n",
    " 'pleasure',\n",
    " 'plenteous',\n",
    " 'plentiful',\n",
    " 'plenty',\n",
    " 'plethora',\n",
    " 'poise',\n",
    " 'polish',\n",
    " 'popular',\n",
    " 'positive',\n",
    " 'powerful',\n",
    " 'prepared',\n",
    " 'pretty',\n",
    " 'principle',\n",
    " 'productive',\n",
    " 'project',\n",
    " 'prominent',\n",
    " 'prosperous',\n",
    " 'protect',\n",
    " 'proud',\n",
    " 'purpose',\n",
    " 'quest',\n",
    " 'quick',\n",
    " 'quiet',\n",
    " 'ready',\n",
    " 'recognize',\n",
    " 'refinement',\n",
    " 'refresh',\n",
    " 'rejoice',\n",
    " 'rejuvenate',\n",
    " 'relax',\n",
    " 'reliance',\n",
    " 'rely',\n",
    " 'remarkable',\n",
    " 'renew',\n",
    " 'renowned',\n",
    " 'replenish',\n",
    " 'resolution',\n",
    " 'resound',\n",
    " 'resources',\n",
    " 'respect',\n",
    " 'restore',\n",
    " 'revere',\n",
    " 'revolutionize',\n",
    " 'rewarding',\n",
    " 'rich',\n",
    " 'robust',\n",
    " 'rousing',\n",
    " 'safe',\n",
    " 'secure',\n",
    " 'see',\n",
    " 'sensation',\n",
    " 'serenity',\n",
    " 'shift',\n",
    " 'shine',\n",
    " 'show',\n",
    " 'silence',\n",
    " 'simple',\n",
    " 'sincerity',\n",
    " 'smart',\n",
    " 'smile',\n",
    " 'smooth',\n",
    " 'solution',\n",
    " 'soul',\n",
    " 'sparkling',\n",
    " 'spirit',\n",
    " 'spirited',\n",
    " 'spiritual',\n",
    " 'splendid',\n",
    " 'spontaneous',\n",
    " 'still',\n",
    " 'stir',\n",
    " 'strong',\n",
    " 'style',\n",
    " 'success',\n",
    " 'sunny',\n",
    " 'support',\n",
    " 'sure',\n",
    " 'surprise',\n",
    " 'sustain',\n",
    " 'synchronized',\n",
    " 'team',\n",
    " 'thankful',\n",
    " 'therapeutic',\n",
    " 'thorough',\n",
    " 'thrilled',\n",
    " 'thrive',\n",
    " 'today',\n",
    " 'together',\n",
    " 'tranquil',\n",
    " 'transform',\n",
    " 'triumph',\n",
    " 'trust',\n",
    " 'truth',\n",
    " 'unity',\n",
    " 'unusual',\n",
    " 'unwavering',\n",
    " 'upbeat',\n",
    " 'value',\n",
    " 'vary',\n",
    " 'venerate',\n",
    " 'venture',\n",
    " 'very',\n",
    " 'vibrant',\n",
    " 'victory',\n",
    " 'vigorous',\n",
    " 'vision',\n",
    " 'visualize',\n",
    " 'vital',\n",
    " 'vivacious',\n",
    " 'voyage',\n",
    " 'wealthy',\n",
    " 'welcome',\n",
    " 'well',\n",
    " 'whole',\n",
    " 'wholesome',\n",
    " 'willing',\n",
    " 'wonder',\n",
    " 'wonderful',\n",
    " 'wondrous',\n",
    " 'xanadu',\n",
    " 'yes',\n",
    " 'yippee',\n",
    " 'young',\n",
    " 'youth',\n",
    " 'youthful',\n",
    " 'zeal',\n",
    " 'zest',\n",
    " 'zing',\n",
    " 'zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 2.64 ms, total: 1.58 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_ast = [phrase.count('*') for phrase in text_data]\n",
    "count_ex = [phrase.count('!') for phrase in text_data]\n",
    "count_qu = [phrase.count('?') for phrase in text_data]\n",
    "count_ar = [phrase.count('@') for phrase in text_data]\n",
    "count_ha = [phrase.count('#') for phrase in text_data]\n",
    "len_pr = [len(phrase) for phrase in text_data]\n",
    "len_max_word = [max([len(x) for x in phrase.split()]) for phrase in text_data]\n",
    "len_min_word = [min([len(x) for x in phrase.split()]) for phrase in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast</th>\n",
       "      <th>ex</th>\n",
       "      <th>qu</th>\n",
       "      <th>ar</th>\n",
       "      <th>ha</th>\n",
       "      <th>len_pr</th>\n",
       "      <th>len_max_word</th>\n",
       "      <th>len_min_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ast  ex  qu  ar  ha  len_pr  len_max_word  len_min_word\n",
       "0    0   1   2   0   0     101             7             2\n",
       "1    0   3   0   0   0     114            17             1\n",
       "2    0   1   0   0   0      86            11             2\n",
       "3    0   0   2   0   0      84             9             2\n",
       "4    0   0   0   0   0      36             7             1\n",
       "5    1   0   0   0   0      20             8             1\n",
       "6    0   0   0   0   0      27            18             3\n",
       "7    0   0   0   0   0      19            19            19\n",
       "8    0   0   0   0   0     120             9             2\n",
       "9    0   0   0   0   0      80             8             1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extra = pd.DataFrame({'ast': count_ast,\n",
    "                          'ex': count_ex,\n",
    "                          'qu': count_qu,\n",
    "                           'ar': count_ar,\n",
    "                          'ha': count_ha,\n",
    "                          'len_pr': len_pr,\n",
    "                          'len_max_word': len_max_word,\n",
    "                          'len_min_word': len_min_word})\n",
    "\n",
    "data_extra.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHARS_TO_REMOVE = '!¬°\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n‚Äú‚Äù‚Äô\\'‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî'\n",
    "MAX_LEN = 2000\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, sw):\n",
    "    def clean_special_chars(text, sw):\n",
    "        text = ' '.join([word for word in text.split() if word.lower() not in sw])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, sw))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data, chars):\n",
    "    def clean_special_chars2(text, chars):\n",
    "        text = ''.join([word for word in text if word not in chars])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars2(x, chars))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 s, sys: 26 ms, total: 8.54 s\n",
      "Wall time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean = preprocess(text_data, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 7.89 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean2 = preprocess2(text_data_clean, CHARS_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data_clean2[:5]\n",
    "\n",
    "# list_len = [len(x.split()) for x in text_data_clean2]\n",
    "# maximo = max(list_len)\n",
    "# print(maximo)\n",
    "\n",
    "# def preprocess3(data, m):\n",
    "#     def clean_special_chars3(text, m):\n",
    "#         text = one_hot(text, m)\n",
    "#         return text\n",
    "\n",
    "#     data = data.astype(str).apply(lambda x: clean_special_chars3(x, m))\n",
    "#     return data\n",
    "\n",
    "# %%time\n",
    "# text_data_clean3 = preprocess3(text_data_clean2, maximo)\n",
    "\n",
    "# text_data_clean3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.14 s, sys: 3.37 ms, total: 8.14 s\n",
      "Wall time: 8.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bad_words = [len(' '.join([word for word in i.split() if word.lower() in arrBad])) for i in text_data_clean2]\n",
    "data_extra['bad_words'] = bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 7.53 ms, total: 12.8 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "good_words = [len(' '.join([word for word in i.split() if word.lower() in arrGood])) for i in text_data_clean2]\n",
    "data_extra['good_words'] = good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extra['bad_ratio'] = data_extra.bad_words / data_extra.len_pr\n",
    "data_extra['good_ratio'] = data_extra.good_words / data_extra.len_pr\n",
    "data_extra['bad_plus_good'] = data_extra.bad_words + data_extra.good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast</th>\n",
       "      <th>ex</th>\n",
       "      <th>qu</th>\n",
       "      <th>ar</th>\n",
       "      <th>ha</th>\n",
       "      <th>len_pr</th>\n",
       "      <th>len_max_word</th>\n",
       "      <th>len_min_word</th>\n",
       "      <th>bad_words</th>\n",
       "      <th>good_words</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "      <th>bad_plus_good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ast  ex  qu  ar  ha  len_pr  len_max_word  len_min_word  bad_words  \\\n",
       "0    0   1   2   0   0     101             7             2          0   \n",
       "1    0   3   0   0   0     114            17             1          0   \n",
       "2    0   1   0   0   0      86            11             2          0   \n",
       "3    0   0   2   0   0      84             9             2          0   \n",
       "4    0   0   0   0   0      36             7             1          0   \n",
       "5    1   0   0   0   0      20             8             1          0   \n",
       "6    0   0   0   0   0      27            18             3          4   \n",
       "7    0   0   0   0   0      19            19            19          0   \n",
       "8    0   0   0   0   0     120             9             2          0   \n",
       "9    0   0   0   0   0      80             8             1          0   \n",
       "\n",
       "   good_words  bad_ratio  good_ratio  bad_plus_good  \n",
       "0           9   0.000000    0.089109              9  \n",
       "1           0   0.000000    0.000000              0  \n",
       "2           0   0.000000    0.000000              0  \n",
       "3           0   0.000000    0.000000              0  \n",
       "4           5   0.000000    0.138889              5  \n",
       "5           0   0.000000    0.000000              0  \n",
       "6           0   0.148148    0.000000              4  \n",
       "7           0   0.000000    0.000000              0  \n",
       "8           3   0.000000    0.025000              3  \n",
       "9          18   0.000000    0.225000             18  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extra.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(max_features=MAX_LEN).fit(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean3 = vectorizer.transform(text_data_clean2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words = pd.DataFrame(text_data_clean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [len(X_words.iloc[:,i].value_counts()) for i in range(X_words.shape[1])]\n",
    "# sel_cols = list()\n",
    "# thr = 0.25\n",
    "\n",
    "# MAX = max(a)\n",
    "\n",
    "# for i in range(X_words.shape[1]):\n",
    "#     vc = len(X_words.iloc[:,i].value_counts())\n",
    "#     if vc/MAX < thr:\n",
    "#         sel_cols.append(i)\n",
    "        \n",
    "# print(len(sel_cols))\n",
    "\n",
    "# X_words2 = X_words.iloc[:,sel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([X_words, data_extra], axis=1)\n",
    "\n",
    "del X_words\n",
    "del data_extra\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>ar</th>\n",
       "      <th>ha</th>\n",
       "      <th>len_pr</th>\n",
       "      <th>len_max_word</th>\n",
       "      <th>len_min_word</th>\n",
       "      <th>bad_words</th>\n",
       "      <th>good_words</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "      <th>bad_plus_good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 2013 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  ar  ha  len_pr  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0   0     101   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0   0     114   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0   0      86   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0   0      84   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0   0      36   \n",
       "\n",
       "   len_max_word  len_min_word  bad_words  good_words  bad_ratio  good_ratio  \\\n",
       "0             7             2          0           9        0.0    0.089109   \n",
       "1            17             1          0           0        0.0    0.000000   \n",
       "2            11             2          0           0        0.0    0.000000   \n",
       "3             9             2          0           0        0.0    0.000000   \n",
       "4             7             1          0           5        0.0    0.138889   \n",
       "\n",
       "   bad_plus_good  \n",
       "0              9  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              5  \n",
       "\n",
       "[5 rows x 2013 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(lower=True)\n",
    "\n",
    "# %%time\n",
    "# tokenizer.fit_on_texts(text_data_clean2)\n",
    "\n",
    "# %%time\n",
    "# X_train = tokenizer.texts_to_sequences(text_data_clean2)\n",
    "\n",
    "# %%time\n",
    "# X_train = pad_sequences(X_train, maxlen=maximo)\n",
    "\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# X_train.head(10)\n",
    "\n",
    "# drop_cols = [i for i in range(MAX_LEN) if len(X_train.iloc[:,i].value_counts()) < 5000]\n",
    "\n",
    "# drop_cols = list()\n",
    "# for i in range(MAX_LEN):\n",
    "#     lgth = len(X_train.iloc[:,i].value_counts())\n",
    "#     if lgth == 1:\n",
    "#         drop_cols.append(str(i))\n",
    "\n",
    "# X_train.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[ft_sel]\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "train_ids = X_train.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.07,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':0.7,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':0.5,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0,\n",
    "    'metric_freq':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.860632\n",
      "[100]\tvalid_0's auc: 0.869627\n",
      "[150]\tvalid_0's auc: 0.870354\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid_0's auc: 0.870613\n",
      "Fold 2\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.862622\n",
      "[100]\tvalid_0's auc: 0.869741\n",
      "[150]\tvalid_0's auc: 0.870172\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's auc: 0.870694\n",
      "Fold 3\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.864851\n",
      "[100]\tvalid_0's auc: 0.871041\n",
      "[150]\tvalid_0's auc: 0.870198\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's auc: 0.871245\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=50,\n",
    "                  early_stopping_rounds=50)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>len_pr</td>\n",
       "      <td>146.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1521</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>len_max_word</td>\n",
       "      <td>81.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>good_ratio</td>\n",
       "      <td>76.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>608</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>910</td>\n",
       "      <td>61.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>1165</td>\n",
       "      <td>55.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1312</td>\n",
       "      <td>55.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>950</td>\n",
       "      <td>55.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>1214</td>\n",
       "      <td>54.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>1363</td>\n",
       "      <td>52.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>bad_ratio</td>\n",
       "      <td>50.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>683</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>1693</td>\n",
       "      <td>45.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1391</td>\n",
       "      <td>44.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>ex</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>1325</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>1071</td>\n",
       "      <td>40.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>855</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>1927</td>\n",
       "      <td>39.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1182</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>1932</td>\n",
       "      <td>38.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>38.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>good_words</td>\n",
       "      <td>37.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>1080</td>\n",
       "      <td>36.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>904</td>\n",
       "      <td>36.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>1877</td>\n",
       "      <td>36.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>231</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>912</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "2005        len_pr  146.666667\n",
       "1521          1521   96.000000\n",
       "2006  len_max_word   81.666667\n",
       "2011    good_ratio   76.333333\n",
       "608            608   75.000000\n",
       "910            910   61.666667\n",
       "1165          1165   55.666667\n",
       "1312          1312   55.666667\n",
       "950            950   55.333333\n",
       "1214          1214   54.666667\n",
       "1363          1363   52.333333\n",
       "2010     bad_ratio   50.333333\n",
       "683            683   50.000000\n",
       "503            503   46.000000\n",
       "1693          1693   45.666667\n",
       "1391          1391   44.333333\n",
       "2001            ex   43.000000\n",
       "1325          1325   41.000000\n",
       "1071          1071   40.333333\n",
       "855            855   40.000000\n",
       "1927          1927   39.333333\n",
       "1182          1182   39.000000\n",
       "1932          1932   38.666667\n",
       "162            162   38.333333\n",
       "2009    good_words   37.333333\n",
       "1080          1080   36.666667\n",
       "904            904   36.666667\n",
       "1877          1877   36.333333\n",
       "231            231   36.000000\n",
       "912            912   35.000000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas = X_train_red.columns\n",
    "imp = pd.DataFrame({'feature': columnas, 'importance': ft_importances/k})\n",
    "df_imp_sort = imp.sort_values('importance', ascending=False)\n",
    "\n",
    "df_imp_sort.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sel = df_imp_sort['feature'].values[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "X_sel = X_train.loc[:,ft_sel]\n",
    "train_ids = X_sel.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.07,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':0.3,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':0.8,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0,\n",
    "    'metric_freq':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(preds, train_data):\n",
    "    r = recall_score([1 if i >=0.5 else 0 for i in train_data], preds, average='macro')  \n",
    "    return 'recall', r, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.86476\n",
      "[100]\tvalid_0's auc: 0.873429\n",
      "[150]\tvalid_0's auc: 0.874694\n",
      "[200]\tvalid_0's auc: 0.873697\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's auc: 0.874863\n",
      "Fold 2\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.866776\n",
      "[100]\tvalid_0's auc: 0.873929\n",
      "[150]\tvalid_0's auc: 0.875319\n",
      "[200]\tvalid_0's auc: 0.874588\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's auc: 0.875442\n",
      "Fold 3\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.867859\n",
      "[100]\tvalid_0's auc: 0.875879\n",
      "[150]\tvalid_0's auc: 0.876154\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid_0's auc: 0.876639\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_sel.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_sel.iloc[train_index, :], X_sel.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    we = weights[train_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=50,\n",
    "                  early_stopping_rounds=50,\n",
    "#                  eval_metric=recall\n",
    "                 )\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_auroc',\n",
    "                           min_delta=0.0,\n",
    "                           patience=1,\n",
    "                           verbose=0,\n",
    "                           mode='max',\n",
    "                           restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[auroc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=30, batch_size=1024, verbose=1, validation_split=0.33,\n",
    "                           callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
