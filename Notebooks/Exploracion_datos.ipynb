{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot, hashing_trick, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import pipeline\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "if not os.path.exists('../logs/'):\n",
    "    os.makedirs('../logs/')\n",
    "\n",
    "NAME = 'Exploracion'\n",
    "    \n",
    "LOG_NAME = '../logs/{}_{}.log'.format(datetime.datetime.now().strftime(\"%Y%m%d\"), NAME)\n",
    "logging.basicConfig(filename=LOG_NAME, level=logging.WARNING, format='%(asctime)s %(message)s')\n",
    "\n",
    "logging.warning(\"\")\n",
    "logging.warning(\"Comienzo script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', usecols=['comment_text', 'target'], nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train2 = dd.read_csv('../data/train.csv', usecols=['comment_text', 'target']).head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.457627</td>\n",
       "      <td>hahahahahahahahhha suck it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>The ranchers seem motivated by mostly by greed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>It was a great show. Not a combo I'd of expect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                       comment_text\n",
       "0  0.000000  This is so cool. It's like, 'would you want yo...\n",
       "1  0.000000  Thank you!! This would make my life a lot less...\n",
       "2  0.000000  This is such an urgent design problem; kudos t...\n",
       "3  0.000000  Is this something I'll be able to install on m...\n",
       "4  0.893617               haha you guys are a bunch of losers.\n",
       "5  0.666667                               ur a sh*tty comment.\n",
       "6  0.457627                        hahahahahahahahhha suck it.\n",
       "7  0.000000                                FFFFUUUUUUUUUUUUUUU\n",
       "8  0.000000  The ranchers seem motivated by mostly by greed...\n",
       "9  0.000000  It was a great show. Not a combo I'd of expect..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "5                                 ur a sh*tty comment.\n",
       "6                          hahahahahahahahhha suck it.\n",
       "7                                  FFFFUUUUUUUUUUUUUUU\n",
       "8    The ranchers seem motivated by mostly by greed...\n",
       "9    It was a great show. Not a combo I'd of expect...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = train['comment_text']\n",
    "text_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 714 ms, sys: 39 µs, total: 714 ms\n",
      "Wall time: 712 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_ast = [phrase.count('*') for phrase in text_data]\n",
    "count_ex = [phrase.count('!') for phrase in text_data]\n",
    "count_qu = [phrase.count('?') for phrase in text_data]\n",
    "len_pr = [len(phrase) for phrase in text_data]\n",
    "len_max_word = [max([len(x) for x in phrase.split()]) for phrase in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast</th>\n",
       "      <th>ex</th>\n",
       "      <th>qu</th>\n",
       "      <th>len_pr</th>\n",
       "      <th>len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ast  ex  qu  len_pr  len_word\n",
       "0    0   1   2     101         7\n",
       "1    0   3   0     114        17\n",
       "2    0   1   0      86        11\n",
       "3    0   0   2      84         9\n",
       "4    0   0   0      36         7\n",
       "5    1   0   0      20         8\n",
       "6    0   0   0      27        18\n",
       "7    0   0   0      19        19\n",
       "8    0   0   0     120         9\n",
       "9    0   0   0      80         8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extra = pd.DataFrame({'ast': count_ast,\n",
    "                          'ex': count_ex,\n",
    "                          'qu': count_qu,\n",
    "                          'len_pr': len_pr,\n",
    "                          'len_word': len_max_word})\n",
    "\n",
    "data_extra.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHARS_TO_REMOVE = '!¡\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "MAX_LEN = 1000\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, sw):\n",
    "    def clean_special_chars(text, sw):\n",
    "        text = ' '.join([word for word in text.split() if word.lower() not in sw])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, sw))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data, chars):\n",
    "    def clean_special_chars2(text, chars):\n",
    "        text = ''.join([word for word in text if word not in chars])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars2(x, chars))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.38 s, sys: 15.9 ms, total: 6.39 s\n",
      "Wall time: 6.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean = preprocess(text_data, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 4.03 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean2 = preprocess2(text_data_clean, CHARS_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_len = [len(x.split()) for x in text_data_clean2]\n",
    "maximo = max(list_len)\n",
    "print(maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess3(data, m):\n",
    "    def clean_special_chars3(text, m):\n",
    "        text = one_hot(text, m)\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars3(x, m))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_data_clean3 = preprocess3(text_data_clean2, maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000).fit(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean3 = vectorizer.transform(text_data_clean2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words = pd.DataFrame(text_data_clean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2505)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([X_words, data_extra], axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2495</th>\n",
       "      <th>2496</th>\n",
       "      <th>2497</th>\n",
       "      <th>2498</th>\n",
       "      <th>2499</th>\n",
       "      <th>ast</th>\n",
       "      <th>ex</th>\n",
       "      <th>qu</th>\n",
       "      <th>len_pr</th>\n",
       "      <th>len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  2495  2496  2497  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   2498  2499  ast  ex  qu  len_pr  len_word  \n",
       "0   0.0   0.0    0   1   2     101         7  \n",
       "1   0.0   0.0    0   3   0     114        17  \n",
       "2   0.0   0.0    0   1   0      86        11  \n",
       "3   0.0   0.0    0   0   2      84         9  \n",
       "4   0.0   0.0    0   0   0      36         7  \n",
       "\n",
       "[5 rows x 2505 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = tokenizer.texts_to_sequences(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = pad_sequences(X_train, maxlen=maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = [i for i in range(MAX_LEN) if len(X_train.iloc[:,i].value_counts()) < 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = list()\n",
    "# for i in range(MAX_LEN):\n",
    "#     lgth = len(X_train.iloc[:,i].value_counts())\n",
    "#     if lgth == 1:\n",
    "#         drop_cols.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 37)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[ft_sel]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "train_ids = X_train.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.07,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':0.3,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':0.8,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.772819\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's auc: 0.773137\n",
      "Fold 2\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.767803\n",
      "[100]\tvalid_0's auc: 0.769148\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's auc: 0.769967\n",
      "Fold 3\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.772458\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's auc: 0.772985\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=50,\n",
    "                  early_stopping_rounds=50)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>len_pr</td>\n",
       "      <td>148.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>len_word</td>\n",
       "      <td>120.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1021</td>\n",
       "      <td>46.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1450</td>\n",
       "      <td>42.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>40.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>1693</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>1683</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>ex</td>\n",
       "      <td>37.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>1108</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2003</td>\n",
       "      <td>35.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>325</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>qu</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>812</td>\n",
       "      <td>33.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>863</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>31.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>1932</td>\n",
       "      <td>31.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>2391</td>\n",
       "      <td>30.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>30.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>2021</td>\n",
       "      <td>29.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>28.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>1355</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>2071</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>1927</td>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>864</td>\n",
       "      <td>27.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>2412</td>\n",
       "      <td>27.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1263</td>\n",
       "      <td>27.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>2241</td>\n",
       "      <td>26.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  importance\n",
       "2503    len_pr  148.333333\n",
       "2504  len_word  120.666667\n",
       "1021      1021   46.666667\n",
       "1450      1450   42.666667\n",
       "410        410   40.333333\n",
       "1693      1693   40.000000\n",
       "1683      1683   38.000000\n",
       "2501        ex   37.333333\n",
       "1108      1108   36.000000\n",
       "2003      2003   35.666667\n",
       "325        325   34.000000\n",
       "2502        qu   34.000000\n",
       "812        812   33.666667\n",
       "863        863   33.333333\n",
       "235        235   31.666667\n",
       "1932      1932   31.666667\n",
       "1665      1665   31.000000\n",
       "2391      2391   30.333333\n",
       "165        165   30.333333\n",
       "2021      2021   29.333333\n",
       "12          12   28.666667\n",
       "1355      1355   28.000000\n",
       "2071      2071   28.000000\n",
       "1927      1927   27.666667\n",
       "864        864   27.333333\n",
       "2412      2412   27.333333\n",
       "1263      1263   27.333333\n",
       "80          80   27.000000\n",
       "182        182   27.000000\n",
       "2241      2241   26.666667"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas = X_train.columns\n",
    "imp = pd.DataFrame({'feature': columnas, 'importance': ft_importances/k})\n",
    "df_imp_sort = imp.sort_values('importance', ascending=False)\n",
    "\n",
    "df_imp_sort.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_sort.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['len_pr', 'len_word', 1021, 1450, 410]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_sel = list()\n",
    "for i, j in zip(df_imp_sort['feature'], df_imp_sort['importance']):\n",
    "    if j > 10:\n",
    "        ft_sel.append(i)\n",
    "        \n",
    "ft_sel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_auroc',\n",
    "                           min_delta=0.0,\n",
    "                           patience=1,\n",
    "                           verbose=0,\n",
    "                           mode='max',\n",
    "                           restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[auroc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=30, batch_size=1024, verbose=1, validation_split=0.33,\n",
    "                           callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ayesa1/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-46-0345cd89dcf8>:2: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ayesa1/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 33500 samples, validate on 16500 samples\n",
      "Epoch 1/30\n",
      "33500/33500 [==============================] - 5s 149us/step - loss: 0.4340 - auroc: 0.5160 - val_loss: 0.2644 - val_auroc: 0.5440\n",
      "Epoch 2/30\n",
      "33500/33500 [==============================] - 4s 131us/step - loss: 0.2720 - auroc: 0.5534 - val_loss: 0.2529 - val_auroc: 0.5737\n",
      "Epoch 3/30\n",
      "33500/33500 [==============================] - 4s 128us/step - loss: 0.2538 - auroc: 0.5929 - val_loss: 0.2473 - val_auroc: 0.6088\n",
      "Epoch 4/30\n",
      "33500/33500 [==============================] - 4s 132us/step - loss: 0.2458 - auroc: 0.6409 - val_loss: 0.2392 - val_auroc: 0.6651\n",
      "Epoch 5/30\n",
      "33500/33500 [==============================] - 4s 130us/step - loss: 0.2313 - auroc: 0.7286 - val_loss: 0.2332 - val_auroc: 0.6878\n",
      "Epoch 6/30\n",
      "33500/33500 [==============================] - 4s 130us/step - loss: 0.2250 - auroc: 0.7786 - val_loss: 0.2327 - val_auroc: 0.7091\n",
      "Epoch 7/30\n",
      "33500/33500 [==============================] - 4s 134us/step - loss: 0.2072 - auroc: 0.8308 - val_loss: 0.2524 - val_auroc: 0.7214\n",
      "Epoch 8/30\n",
      "33500/33500 [==============================] - 4s 130us/step - loss: 0.2001 - auroc: 0.8411 - val_loss: 0.2373 - val_auroc: 0.7471\n",
      "Epoch 9/30\n",
      "33500/33500 [==============================] - 4s 130us/step - loss: 0.1974 - auroc: 0.8548 - val_loss: 0.2301 - val_auroc: 0.7389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f783120cf98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(stop_words='english', strip_accents='unicode')\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def tokenize_corpus(td, mode='d'):\n",
    "    for t in td:\n",
    "        tokens = analyzer(t)\n",
    "        if mode == 'd':\n",
    "            yield tokens\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = tokenize_corpus(text_data_clean, mode='t')\n",
    "dist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_words = dict(dist.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = list()\n",
    "k = mc_words.keys()\n",
    "for phrase in text_data:\n",
    "    line = [1 if word in phrase else 0 for word in k]\n",
    "    X_train.append(np.asarray(line))\n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del text_data\n",
    "# del tokens\n",
    "# del dist\n",
    "# del mc_words\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "train_ids = X_train.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.1,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':1,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':1,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=10,\n",
    "                  early_stopping_rounds=20)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    analyzer='word',\n",
    "    stop_words = 'english',\n",
    "    ngram_range = (1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('LGBM', lgb.LGBMClassifier(metric='auc', e))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas de Texto\n",
    "\n",
    "En este apartado vamos a analizar con ayuda de NLTK como es la distribución del texto a lo largo de todo el corpus. Estaremos interesados en caracteríticas como tokens más frecuentes, longitud del corpus, longitud del vocabulario, etcétera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(stop_words='english', strip_accents='unicode')\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def tokenize_corpus(mode='d'):\n",
    "    for t in text_data:\n",
    "        tokens = analyzer(t)\n",
    "        if mode == 'd':\n",
    "            yield tokens\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de Frecuencias\n",
    "\n",
    "La mayoría de herramientas que trabajan con texto en python (NLTK, gensim, Scikit Learn...) necesitan manejar una estructura de datos en la que se implementa una distribución de frecuencias que da lugar a una representación conocida como **Bag of Words (BoW)** en la que simplemente, por cada documento o a nivel global del corpus, se mantiene un contador con el número de apariciones de cada palabra o token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = tokenize_corpus(mode='t')\n",
    "dist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_words = dict(dist.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = list()\n",
    "for phrase in text_data:\n",
    "    line = list()\n",
    "    for word in mc_words.keys():\n",
    "        if word in phrase:\n",
    "            line.append(1)\n",
    "        else:\n",
    "            line.append(0)\n",
    "    X.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dist.most_common(100))\n",
    "df.columns = ['Token', 'Frecuencia']\n",
    "df.sort_values('Frecuencia')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dist.most_common()[-100:])\n",
    "df.columns = ['Token', 'Frecuencia']\n",
    "df.sort_values('Frecuencia')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diseñando Nuestro Modelo\n",
    "\n",
    "## Diccionario\n",
    "\n",
    "Nuestro modelo de tópicos estará basado en una representación BoW del corpus. Únicamente tendremos en cuenta la frecuencia global de los términos y no una frecuencia de documentos tipo TF-IDF. Lo primero que necesitamos construir es un diccionario con nuestro vocabulario. Empezaremos con un vocabulario sin filtros, para comprobar que resultamos obtenemos y si nuestro estudio previo ha tenido sentido a la hora de ayudarnos con el filtrado posterior.\n",
    "\n",
    "Empezamos a utilizar gensim para construir el diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = tokenize_corpus()\n",
    "%time dictionary = gensim.corpora.Dictionary(stream)\n",
    "dictionary.save('../data/original.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[dictionary.num_docs, dictionary.num_pos, len(dictionary.token2id)]]\n",
    "df = pd.DataFrame(data)\n",
    "df.columns=['Numero de frases analizadas', 'Numero de tokens analizados', 'Numero de tokens únicos actuales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Necesitamos declarar un iterable para acceder en streaming a la representación BoW de cada uno de nuestros documentos (comentarios). Este iterable será utilizado de forma eficiente por gensim para entrenar el modelo de forma iterativa en un número determinado de pasadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCorpus(object):\n",
    "\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.__path = path\n",
    "        self.__dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in tokenize_corpus(self.__path):\n",
    "            yield self.__dictionary.doc2bow(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms\n",
    "\n",
    "def print_lda_model(lda_model, num_topics=20):\n",
    "    topic_summaries = []\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    for i in range(num_topics):\n",
    "        print('\\n')\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [x for x in dist.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(text_data.values)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in tokens_list.decode()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = gensim.corpora.Dictionary.load('../data/original.dict')\n",
    "# corpus = MovieCorpus('../data/original.dict', text_data)\n",
    "# gensim.corpora.MmCorpus.serialize('../data/corpus.mm', corpus)\n",
    "# corpus = gensim.corpora.MmCorpus('../data/corpus.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando Modelo LDA\n",
    "\n",
    "Necesitamos poder comparar los distintos modelos que vamos a ir generando para poder comprobar que van mejorando con las acciones que tomamos. Existen muchas diversas formas de evaluar un modelo LDA, cualquiera compatible con evaluar clusters procedentes de algoritmos de clustering.\n",
    "\n",
    "Los clusters se suelen evaluar midiendo la coherencia de sus componentes. En nuestro caso concreto, cada tópico tendrá mayor calidad si:\n",
    "\n",
    "* Los documentos dominados por los mismos tópicos han de ser similares entre si\n",
    "* Los documentos dominados por tópicos diferentes y poco solapados han de ser distintos entre si\n",
    "\n",
    "Afortunadamente gensim proporciona sus propias herramientas para medir la coherencia que usamos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando Tokens Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc100 = [mc[0] for mc in dist.most_common(100)]\n",
    "terms_id = lda_model.get_topic_terms(2)\n",
    "terms_str = [dictionary.id2token[id[0]] for id in terms_id if id[0] in dictionary.id2token]\n",
    "list(set(mc100) & set(terms_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_filter_most_frequent(dictionary, dist, n=200):\n",
    "    most_common = dist.most_common(n)\n",
    "    mc_ids = [dictionary.token2id[t[0]] for t in most_common]\n",
    "    dictionary.filter_tokens(bad_ids=mc_ids)\n",
    "    dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Longitud del vocabulario actual: {}'.format(len(dictionary.token2id)))\n",
    "dictionary_filter_most_frequent(dictionary, dist)\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "print('Longitud del vocabulario Filtrado: {}'.format(len(dictionary.token2id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando Tokens\n",
    "\n",
    "Además del lowercase, vamos a eliminar también los plurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dictionary(dictionary):\n",
    "    from textblob import Word\n",
    "    plurals = []\n",
    "    for token in dictionary.values():\n",
    "        if token.endswith('s'):\n",
    "            singular = Word(token).singularize()\n",
    "            if token != singular:\n",
    "                singular_id = dictionary.token2id.get(singular, None)\n",
    "                if singular_id:\n",
    "                    plurals.append(dictionary.token2id[token])\n",
    "                    \n",
    "    dictionary.filter_tokens(bad_ids=plurals)\n",
    "    dictionary.compactify()\n",
    "    return plurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numero de tokens únicos actuales: {}'.format(len(dictionary.token2id)))\n",
    "plurals = normalize_dictionary(dictionary)\n",
    "print('Numero de plurales detectados: {}'.format(len(plurals)))\n",
    "print('Numero de tokens únicos actuales: {}'.format(len(dictionary.token2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('normalized.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus1.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus1.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitando el Vocabulario por Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_keep_n_frequent(dictionary, dist, n=5000):\n",
    "    tokens_by_freq = dist.most_common(len(dist))\n",
    "    mf = []\n",
    "    for t in tokens_by_freq:\n",
    "        id = dictionary.token2id.get(t[0], None)\n",
    "        if id:\n",
    "            mf.append(id)\n",
    "            if len(mf) == n:\n",
    "                break\n",
    "    dictionary.filter_tokens(good_ids=mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus3.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus3.mm')\n",
    "%time lda_model= gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "print_lda_model(lda_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda_model= gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word=dictionary)\n",
    "print_lda_model(lda_model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limite de Reviews por Película\n",
    "\n",
    "Un primer filtro que podemos aplicar para evitar bias es un límite sobre el número de revies para una misma película. Usaremos un parámetro configurable con un valor inicial de 10 después de estudiar la primera gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_by_path = {}\n",
    "for urls_file in walk_corpus('./resources/aclImdb/all/', 'urls.urls'):\n",
    "    dirname = os.path.dirname(urls_file)\n",
    "    with open(urls_file) as f:\n",
    "            ids_map = {}\n",
    "            lines = f.readlines()\n",
    "            for index, line in enumerate(lines):\n",
    "                movie_id = id_pattern.search(line).group(1)\n",
    "                ids_map[index] = movie_id\n",
    "            ids_by_path[dirname] = ids_map\n",
    "            \n",
    "line_id_pattern = re.compile('([0-9]+)_[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(path, pattern, min_df=1, mode='d', limit=10):\n",
    "    movie_counter = Counter()\n",
    "\n",
    "    for corpus_file in walk_corpus(path, pattern):\n",
    "        dirname = os.path.dirname(corpus_file)\n",
    "        line_id = int(line_id_pattern.search(corpus_file).group(1))\n",
    "        ids_map = ids_by_path[dirname]\n",
    "        movie_id = ids_map[line_id]\n",
    "        if movie_counter[movie_id] <= limit:\n",
    "            movie_counter[movie_id] += 1\n",
    "            with open(corpus_file, 'r') as next_file:\n",
    "                next_review = next_file.read()\n",
    "                tokens = analyzer(next_review)\n",
    "                if mode == 'd':\n",
    "                    yield tokens\n",
    "                else:\n",
    "                    for token in tokens:\n",
    "                        yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dist_limited = FreqDist(tokenize_corpus('./resources/aclImdb/all/', '*.txt', mode='t'))\n",
    "print(dist_limited)\n",
    "pp.pprint(dist_limited.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dictionary = gensim.corpora.Dictionary(tokenize_corpus('./resources/aclImdb/all/', '*.txt'))\n",
    "data = [[dictionary.num_docs, dictionary.num_pos, len(dictionary.token2id)]]\n",
    "df = pd.DataFrame(data)\n",
    "df.columns=['Numero de reviews analizadas', 'Numero de tokens analizados', 'Numero de tokens únicos actuales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('limited.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando normalización y filtrado de vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_filter_most_frequent(dictionary, dist_limited)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "plurals = normalize_dictionary(dictionary)\n",
    "dictionary.save('limited.normalized.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus4.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus4.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_keep_n_frequent(dictionary, dist_limited)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus5.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus5.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "lda_model.save('limited.normalized.filtered.model')\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando Vocabulario Polarizado\n",
    "\n",
    "En todos los modelos anteriores hemos visto que existen varios tipos de palabras que concurrente aparecen con bastante frecuencia en varios tópicos, pero que aportan escaso valor a la hora de categorizar por temáticas. Algunos ejemplos de estos tipos de palabras son nombres propios y verbos que podemos filtrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_filter_neutral(dictionary, polarity=0.5):\n",
    "    from textblob import TextBlob\n",
    "    neutrals = []\n",
    "    for token in dictionary.values():\n",
    "        if len(token) > 1:\n",
    "            upper = token[0].upper() + token[1:]\n",
    "        blob = TextBlob(upper)\n",
    "        if abs(blob.polarity) <= polarity and blob.pos_tags[0][1] != 'NNP' and not blob.pos_tags[0][1].startswith('VB'):\n",
    "            neutrals.append(dictionary.token2id[token])\n",
    "                    \n",
    "    dictionary.filter_tokens(good_ids=neutrals)\n",
    "    dictionary.compactify()\n",
    "    return neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "print('Número de palabras iniciales: {}'.format(len(dictionary)))\n",
    "neutrals = dictionary_filter_neutral(dictionary, 0.0)\n",
    "print(\"Número de palabras neutrales: {}\".format(len(neutrals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "dictionary.save('final.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus6.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus6.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('neutral.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando otros Modelos de Representación: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "gensim.corpora.MmCorpus.serialize('corpus7.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus7.mm')\n",
    "%time lda_model_tfidf = gensim.models.ldamodel.LdaModel(tfidf[corpus], num_topics=20, id2word=dictionary)\n",
    "pp.pprint(lda_model_tfidf.print_topics(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Nuestro Modelo como Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "r = requests.get(\"http://www.omdbapi.com/?i=tt0379889&apikey=ccedfaeb\")\n",
    "pp.pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando una Review Positiva\n",
    "\n",
    "Para analizar las reviews, primero vamos a tokenizar el texto y lo vamos a convertir en una representación Bag of Words con proyección a nuestro diccionario. Esta representación es la que podemos pasar a nuestro modelo LDA para que nos devuelva la distribución de tópicos más probable sobre nuestro texto inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('final.dict')\n",
    "good_review_text = \"\"\"I just saw this at the Toronto International Film Festival in the beautiful Elgin Theatre. \n",
    "I was blown away by the beautiful cinematography, the brilliant adaptation of a very tricky play and last \n",
    "but not least, the bravura performance of Al Pacino, who was born to play this role, \n",
    "which was perfectly balanced by an equally strong performance from Jeremy Irons.<br /><br />\n",
    "The film deftly explores the themes of love vs loyalty, law vs justice, and passion vs reason. \n",
    "Some might protest that the content is inherently anti-semitic, \n",
    "however they should consider the historical context of the story, \n",
    "and the delicate and nuanced way in which it is told in this adaptation\"\"\"\n",
    "good_review_tokens = analyzer(good_review_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(good_review_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuales son los 10 tokens más prominentes del tópico asignado con más probabilidad, el **tópico 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_tokens(model, topic_id, n_tokens=10):\n",
    "    terms = model.show_topic(topic_id, n_tokens)\n",
    "    return [item[0] for item in terms]\n",
    "tokens = get_topic_tokens(lda_model, 6, 20)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_tokens = list(set(good_review_tokens) & set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Sentimiento sobre los Keywords de los Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def explore_opinions(text, keywords):\n",
    "    from textblob import TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    data = []\n",
    "    for sentence in blob.sentences:\n",
    "        for token in keywords:\n",
    "            if token in sentence.words:\n",
    "                data.append([token, sentence.__str__(), sentence.sentiment[0], sentence.sentiment[1]])\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['Token', 'Sentence', 'Sentiment Polarity', 'Sentiment Subjectivity']\n",
    "    return df\n",
    "        \n",
    "display(HTML(explore_opinions(good_review_text, shared_tokens).to_html().replace(\"\\\\n\",\"<br>\").replace('adaptation', '<strong>adaptation</strong>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando una Review Negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review_text = \"\"\"I have to admit that although I'm a fan of Shakespeare, \n",
    "I was never really familiar with this play. And what I really can't say is whether this is a poor adaptation, \n",
    "or whether the play is just a bad choice for film. \n",
    "There are some nice pieces of business in it, but the execution is very clunky and the plot is obvious. \n",
    "The theme of the play is on the nature of debt, using the financial idea of debt and justice as a \n",
    "metaphor for emotional questions. That becomes clear when the issue of the rings becomes more important than \n",
    "the business with Shylock, which unfortunately descends into garden variety anti-Semitisim despite \n",
    "the Bard's best attempts to salvage him with a couple nice monologues.<br /><br />\n",
    "Outside of Jeremy Irons' dignified turn, I didn't think there was a decent performance in the bunch. \n",
    "Pacino's Yiddish consists of a slight whine added to the end of every pronouncement, and \n",
    "some of the better Shylock scenes are reduced to variations on the standard \"Pacino gets angry\" \n",
    "scene that his fans know and love. But Lynn Collins is outright embarrassing, to the point where I \n",
    "would have thought they would have screen-tested her right out of the picture early on. \n",
    "When she goes incognito as a man, it's hard not to laugh at all the things we're not supposed to laugh at. \n",
    "With Joseph Fiennes standing there trying to look sincere and complicated, it's hard not to make \n",
    "devastating comparisons to Gwyneth Paltrow's performance in \"Shakespeare in Love.\" \n",
    "The big problem however that over-rides everything in this film is just a lack of emotional focus. \n",
    "It's really hard to tell whether this film is trying to be a somewhat serious comedy or a strangely silly drama. \n",
    "Surely a good summer stock performance would wring more laughs from the material than this somber production. \n",
    "The actors seem embarrassed to be attempting humor, and unsure of where to place dramatic and comedic emphasis. \n",
    "All of this is basically the fault of the director, Michael Radford, who seems to think that the material \n",
    "is a great deal heavier than it appears to me.\"\"\"\n",
    "bad_review_tokens = analyzer(bad_review_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(bad_review_tokens))\n",
    "list(set(bad_review_tokens) & set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(explore_opinions(bad_review_text, shared_tokens).to_html().replace(\"\\\\n\",\"<br>\").replace('adaptation', '<strong>adaptation</strong>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando Reviews Nuevas fuera del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_text = \"\"\"Drug wars, meth, the lot. I thought no thank you. \n",
    "I kept hearing how good it was and I kept saying: \"No thank you\" \n",
    "Last January I got sick, one of those illnesses you can't quite figure out. \n",
    "Maybe it was pre and post election depression, I don't know. But I stayed in bed for almost \n",
    "10 days and then it happened. I saw the first episode and I was immediately and I mean immediately, \n",
    "hooked. I saw the entire series in 9 days. Voraciously. Now I had time to reflect. Why I wonder. \n",
    "When I think about it the first thing that comes to mind is not a thing it's Bryan Cranston. \n",
    "I know the concept was superb as was the writing but Bryan Cranston made it all real. \n",
    "His performance, the creation of Walter White will be studied in the Acting classes of the future. \n",
    "He is the one that pulls you forward - as well as backwards and sideways - then I realized that his \n",
    "creation acquired the power that it acquired, in great part thanks to the extraordinary cast of supporting players. \n",
    "I could write a page for each one of them but I'm just going to mention Aaron Paul. \n",
    "I ended up loving him. I developed a visceral need to see him find a way out. Well, what can I tell you. \n",
    "I know that one day, maybe when my kids are old enough, I shall see \"Breaking Bad\" again. I can't wait.\"\"\"\n",
    "bb_review_tokens = analyzer(bb_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(bb_review_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topic(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_text_2 = \"\"\"What do you get when you have a chemistry teacher in a mid life crisis, dying of cancer, \n",
    "and washing cars as a second job to make ends meet for his middle class family? One of the greatest television \n",
    "dramas of all time with crazy plot twists, brilliant performances, and unforgettable characters and cinematography.\n",
    "There is so much to like about the masterpiece that is Breaking Bad. Take your pick: the acting, \n",
    "the writing, the story lines, the plot, the suspense the cliff hangers, the action scenes, the camera work, \n",
    "the characters, the character arcs, the realism, the satirical style, any season, the end, the casting, the \n",
    "dark humor and humor relief, the scenery, the contrast between background and foreground to establish artistic \n",
    "effect (the sun shiny clear blue skies of the NM desert behind the gruesome organized crime and violence of the \n",
    "underworld), the mixing of favorite genres (crime caper, dark comedy, western, noir, horror, suspense, action, \n",
    "drama, thriller, Shakespearean tragedy, dystopia, psychological character study..), the lines/quotes...\n",
    "the list goes on.\n",
    "What's amazing about Breaking Bad is it begins so humble and quiet, and as it continues to let its' story unfold,\n",
    "it explodes. It gets better and better each season until the end in the final season, we don't know if we're watching a\n",
    "television show or an Academy Award winning motion picture. The show dares to go where no one would have thought \n",
    "it would go- into a transcendent realm of classic cinema- and it pulls it off beautifully.\"\"\"\n",
    "bb_review_tokens = analyzer(bb_text_2)\n",
    "lda_model.get_document_topics(lda_model.id2word.doc2bow(bb_review_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 2))))\n",
    "lda_model.show_topic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 4))))\n",
    "lda_model.show_topic(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 18))))\n",
    "lda_model.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
