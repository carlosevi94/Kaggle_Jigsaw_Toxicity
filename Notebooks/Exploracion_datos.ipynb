{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot, hashing_trick, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import pipeline\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "if not os.path.exists('../logs/'):\n",
    "    os.makedirs('../logs/')\n",
    "\n",
    "NAME = 'Exploracion'\n",
    "    \n",
    "LOG_NAME = '../logs/{}_{}.log'.format(datetime.datetime.now().strftime(\"%Y%m%d\"), NAME)\n",
    "logging.basicConfig(filename=LOG_NAME, level=logging.WARNING, format='%(asctime)s %(message)s')\n",
    "\n",
    "logging.warning(\"\")\n",
    "logging.warning(\"Comienzo script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', usecols=['comment_text', 'target'], nrows=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.457627</td>\n",
       "      <td>hahahahahahahahhha suck it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>The ranchers seem motivated by mostly by greed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>It was a great show. Not a combo I'd of expect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                       comment_text\n",
       "0  0.000000  This is so cool. It's like, 'would you want yo...\n",
       "1  0.000000  Thank you!! This would make my life a lot less...\n",
       "2  0.000000  This is such an urgent design problem; kudos t...\n",
       "3  0.000000  Is this something I'll be able to install on m...\n",
       "4  0.893617               haha you guys are a bunch of losers.\n",
       "5  0.666667                               ur a sh*tty comment.\n",
       "6  0.457627                        hahahahahahahahhha suck it.\n",
       "7  0.000000                                FFFFUUUUUUUUUUUUUUU\n",
       "8  0.000000  The ranchers seem motivated by mostly by greed...\n",
       "9  0.000000  It was a great show. Not a combo I'd of expect..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "5                                 ur a sh*tty comment.\n",
       "6                          hahahahahahahahhha suck it.\n",
       "7                                  FFFFUUUUUUUUUUUUUUU\n",
       "8    The ranchers seem motivated by mostly by greed...\n",
       "9    It was a great show. Not a combo I'd of expect...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = train['comment_text']\n",
    "text_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "MAX_LEN = 1000\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, sw):\n",
    "    def clean_special_chars(text, sw):\n",
    "        text = ' '.join([word for word in text.split() if word.lower() not in sw])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, sw))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data, chars):\n",
    "    def clean_special_chars2(text, chars):\n",
    "        text = ''.join([word for word in text if word not in chars])\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars2(x, chars))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.7 s, sys: 47.6 ms, total: 30.7 s\n",
      "Wall time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean = preprocess(text_data, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.66 s, sys: 23.8 ms, total: 4.69 s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean2 = preprocess2(text_data_clean, CHARS_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cool like would want mother read this Really g...\n",
       "1    Thank you would make life lot less anxietyindu...\n",
       "2     urgent design problem kudos taking on impressive\n",
       "3         something Ill able install site releasing it\n",
       "4                               haha guys bunch losers\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_clean2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "list_len = [len(x.split()) for x in text_data_clean2]\n",
    "maximo = max(list_len)\n",
    "print(maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess3(data, m):\n",
    "    def clean_special_chars3(text, m):\n",
    "        text = one_hot(text, m)\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars3(x, m))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.62 s, sys: 48 ms, total: 7.67 s\n",
      "Wall time: 7.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data_clean3 = preprocess3(text_data_clean2, maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [104, 106, 42, 106, 113, 115, 42, 139, 80, 15,...\n",
       "1    [19, 46, 42, 67, 158, 13, 144, 3, 98, 70, 144,...\n",
       "2                      [158, 46, 41, 65, 175, 117, 20]\n",
       "3                       [29, 2, 18, 128, 170, 102, 70]\n",
       "4                                   [151, 76, 156, 81]\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_clean3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean3 = vectorizer.transform(text_data_clean2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(text_data_clean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = tokenizer.texts_to_sequences(text_data_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = pad_sequences(X_train, maxlen=maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = [i for i in range(MAX_LEN) if len(X_train.iloc[:,i].value_counts()) < 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = list()\n",
    "# for i in range(MAX_LEN):\n",
    "#     lgth = len(X_train.iloc[:,i].value_counts())\n",
    "#     if lgth == 1:\n",
    "#         drop_cols.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "train_ids = X_train.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.1,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':0.5,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':0.8,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c05d2350e164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2157\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m             \u001b[0;31m# if the dim was reduced, then pass a lower-dim the next time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2223\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2198\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take\u001b[0;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[1;32m   3357\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   3358\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3359\u001b[0;31m                                    verify=True)\n\u001b[0m\u001b[1;32m   3360\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m-> 1350\u001b[0;31m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[1;32m   1234\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[0;32m-> 1235\u001b[0;31m                 for blk in self.blocks]\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[1;32m   1234\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[0;32m-> 1235\u001b[0;31m                 for blk in self.blocks]\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1236\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[0;32m-> 1238\u001b[0;31m                                        allow_fill=True, fill_value=fill_value)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=10,\n",
    "                  early_stopping_rounds=20)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = X_train.columns\n",
    "imp = pd.DataFrame({'feature': columnas, 'importance': ft_importances/k})\n",
    "df_imp_sort = imp.sort_values('importance', ascending=False)\n",
    "\n",
    "df_imp_sort.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_sort.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_auroc',\n",
    "                           min_delta=0.0,\n",
    "                           patience=1,\n",
    "                           verbose=0,\n",
    "                           mode='max',\n",
    "                           restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[auroc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=30, batch_size=1024, verbose=1, validation_split=0.33,\n",
    "                           callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(stop_words='english', strip_accents='unicode')\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def tokenize_corpus(td, mode='d'):\n",
    "    for t in td:\n",
    "        tokens = analyzer(t)\n",
    "        if mode == 'd':\n",
    "            yield tokens\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = tokenize_corpus(text_data_clean, mode='t')\n",
    "dist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_words = dict(dist.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = list()\n",
    "k = mc_words.keys()\n",
    "for phrase in text_data:\n",
    "    line = [1 if word in phrase else 0 for word in k]\n",
    "    X_train.append(np.asarray(line))\n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del text_data\n",
    "# del tokens\n",
    "# del dist\n",
    "# del mc_words\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "train_ids = X_train.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'min_data_in_leaf':20,\n",
    "        'max_depth':-1,\n",
    "        'metric':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.1,\n",
    "        'num_leaves':75,\n",
    "        'colsample_bytree':1,\n",
    "        'objective':'binary',\n",
    "        'n_jobs':-1,\n",
    "        'seed':42,\n",
    "        'bagging_fraction':1,\n",
    "        'lambda_l1':0,\n",
    "        'lambda_l2':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "ft_importances = np.zeros(X_train.shape[1])\n",
    "\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(train_ids, y_train):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "\n",
    "    X_fit, X_val = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_fit, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=10,\n",
    "                  early_stopping_rounds=20)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "\n",
    "    ft_importances += lgb_model.feature_importances_\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    analyzer='word',\n",
    "    stop_words = 'english',\n",
    "    ngram_range = (1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('LGBM', lgb.LGBMClassifier(metric='auc', e))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas de Texto\n",
    "\n",
    "En este apartado vamos a analizar con ayuda de NLTK como es la distribución del texto a lo largo de todo el corpus. Estaremos interesados en caracteríticas como tokens más frecuentes, longitud del corpus, longitud del vocabulario, etcétera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(stop_words='english', strip_accents='unicode')\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def tokenize_corpus(mode='d'):\n",
    "    for t in text_data:\n",
    "        tokens = analyzer(t)\n",
    "        if mode == 'd':\n",
    "            yield tokens\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de Frecuencias\n",
    "\n",
    "La mayoría de herramientas que trabajan con texto en python (NLTK, gensim, Scikit Learn...) necesitan manejar una estructura de datos en la que se implementa una distribución de frecuencias que da lugar a una representación conocida como **Bag of Words (BoW)** en la que simplemente, por cada documento o a nivel global del corpus, se mantiene un contador con el número de apariciones de cada palabra o token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = tokenize_corpus(mode='t')\n",
    "dist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_words = dict(dist.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = list()\n",
    "for phrase in text_data:\n",
    "    line = list()\n",
    "    for word in mc_words.keys():\n",
    "        if word in phrase:\n",
    "            line.append(1)\n",
    "        else:\n",
    "            line.append(0)\n",
    "    X.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dist.most_common(100))\n",
    "df.columns = ['Token', 'Frecuencia']\n",
    "df.sort_values('Frecuencia')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dist.most_common()[-100:])\n",
    "df.columns = ['Token', 'Frecuencia']\n",
    "df.sort_values('Frecuencia')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diseñando Nuestro Modelo\n",
    "\n",
    "## Diccionario\n",
    "\n",
    "Nuestro modelo de tópicos estará basado en una representación BoW del corpus. Únicamente tendremos en cuenta la frecuencia global de los términos y no una frecuencia de documentos tipo TF-IDF. Lo primero que necesitamos construir es un diccionario con nuestro vocabulario. Empezaremos con un vocabulario sin filtros, para comprobar que resultamos obtenemos y si nuestro estudio previo ha tenido sentido a la hora de ayudarnos con el filtrado posterior.\n",
    "\n",
    "Empezamos a utilizar gensim para construir el diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = tokenize_corpus()\n",
    "%time dictionary = gensim.corpora.Dictionary(stream)\n",
    "dictionary.save('../data/original.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[dictionary.num_docs, dictionary.num_pos, len(dictionary.token2id)]]\n",
    "df = pd.DataFrame(data)\n",
    "df.columns=['Numero de frases analizadas', 'Numero de tokens analizados', 'Numero de tokens únicos actuales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Necesitamos declarar un iterable para acceder en streaming a la representación BoW de cada uno de nuestros documentos (comentarios). Este iterable será utilizado de forma eficiente por gensim para entrenar el modelo de forma iterativa en un número determinado de pasadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCorpus(object):\n",
    "\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.__path = path\n",
    "        self.__dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in tokenize_corpus(self.__path):\n",
    "            yield self.__dictionary.doc2bow(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms\n",
    "\n",
    "def print_lda_model(lda_model, num_topics=20):\n",
    "    topic_summaries = []\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    for i in range(num_topics):\n",
    "        print('\\n')\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [x for x in dist.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(text_data.values)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in tokens_list.decode()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = gensim.corpora.Dictionary.load('../data/original.dict')\n",
    "# corpus = MovieCorpus('../data/original.dict', text_data)\n",
    "# gensim.corpora.MmCorpus.serialize('../data/corpus.mm', corpus)\n",
    "# corpus = gensim.corpora.MmCorpus('../data/corpus.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando Modelo LDA\n",
    "\n",
    "Necesitamos poder comparar los distintos modelos que vamos a ir generando para poder comprobar que van mejorando con las acciones que tomamos. Existen muchas diversas formas de evaluar un modelo LDA, cualquiera compatible con evaluar clusters procedentes de algoritmos de clustering.\n",
    "\n",
    "Los clusters se suelen evaluar midiendo la coherencia de sus componentes. En nuestro caso concreto, cada tópico tendrá mayor calidad si:\n",
    "\n",
    "* Los documentos dominados por los mismos tópicos han de ser similares entre si\n",
    "* Los documentos dominados por tópicos diferentes y poco solapados han de ser distintos entre si\n",
    "\n",
    "Afortunadamente gensim proporciona sus propias herramientas para medir la coherencia que usamos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando Tokens Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc100 = [mc[0] for mc in dist.most_common(100)]\n",
    "terms_id = lda_model.get_topic_terms(2)\n",
    "terms_str = [dictionary.id2token[id[0]] for id in terms_id if id[0] in dictionary.id2token]\n",
    "list(set(mc100) & set(terms_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_filter_most_frequent(dictionary, dist, n=200):\n",
    "    most_common = dist.most_common(n)\n",
    "    mc_ids = [dictionary.token2id[t[0]] for t in most_common]\n",
    "    dictionary.filter_tokens(bad_ids=mc_ids)\n",
    "    dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Longitud del vocabulario actual: {}'.format(len(dictionary.token2id)))\n",
    "dictionary_filter_most_frequent(dictionary, dist)\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "print('Longitud del vocabulario Filtrado: {}'.format(len(dictionary.token2id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando Tokens\n",
    "\n",
    "Además del lowercase, vamos a eliminar también los plurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dictionary(dictionary):\n",
    "    from textblob import Word\n",
    "    plurals = []\n",
    "    for token in dictionary.values():\n",
    "        if token.endswith('s'):\n",
    "            singular = Word(token).singularize()\n",
    "            if token != singular:\n",
    "                singular_id = dictionary.token2id.get(singular, None)\n",
    "                if singular_id:\n",
    "                    plurals.append(dictionary.token2id[token])\n",
    "                    \n",
    "    dictionary.filter_tokens(bad_ids=plurals)\n",
    "    dictionary.compactify()\n",
    "    return plurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numero de tokens únicos actuales: {}'.format(len(dictionary.token2id)))\n",
    "plurals = normalize_dictionary(dictionary)\n",
    "print('Numero de plurales detectados: {}'.format(len(plurals)))\n",
    "print('Numero de tokens únicos actuales: {}'.format(len(dictionary.token2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('normalized.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus1.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus1.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitando el Vocabulario por Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_keep_n_frequent(dictionary, dist, n=5000):\n",
    "    tokens_by_freq = dist.most_common(len(dist))\n",
    "    mf = []\n",
    "    for t in tokens_by_freq:\n",
    "        id = dictionary.token2id.get(t[0], None)\n",
    "        if id:\n",
    "            mf.append(id)\n",
    "            if len(mf) == n:\n",
    "                break\n",
    "    dictionary.filter_tokens(good_ids=mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus3.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus3.mm')\n",
    "%time lda_model= gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "print_lda_model(lda_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda_model= gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word=dictionary)\n",
    "print_lda_model(lda_model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limite de Reviews por Película\n",
    "\n",
    "Un primer filtro que podemos aplicar para evitar bias es un límite sobre el número de revies para una misma película. Usaremos un parámetro configurable con un valor inicial de 10 después de estudiar la primera gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_by_path = {}\n",
    "for urls_file in walk_corpus('./resources/aclImdb/all/', 'urls.urls'):\n",
    "    dirname = os.path.dirname(urls_file)\n",
    "    with open(urls_file) as f:\n",
    "            ids_map = {}\n",
    "            lines = f.readlines()\n",
    "            for index, line in enumerate(lines):\n",
    "                movie_id = id_pattern.search(line).group(1)\n",
    "                ids_map[index] = movie_id\n",
    "            ids_by_path[dirname] = ids_map\n",
    "            \n",
    "line_id_pattern = re.compile('([0-9]+)_[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(path, pattern, min_df=1, mode='d', limit=10):\n",
    "    movie_counter = Counter()\n",
    "\n",
    "    for corpus_file in walk_corpus(path, pattern):\n",
    "        dirname = os.path.dirname(corpus_file)\n",
    "        line_id = int(line_id_pattern.search(corpus_file).group(1))\n",
    "        ids_map = ids_by_path[dirname]\n",
    "        movie_id = ids_map[line_id]\n",
    "        if movie_counter[movie_id] <= limit:\n",
    "            movie_counter[movie_id] += 1\n",
    "            with open(corpus_file, 'r') as next_file:\n",
    "                next_review = next_file.read()\n",
    "                tokens = analyzer(next_review)\n",
    "                if mode == 'd':\n",
    "                    yield tokens\n",
    "                else:\n",
    "                    for token in tokens:\n",
    "                        yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dist_limited = FreqDist(tokenize_corpus('./resources/aclImdb/all/', '*.txt', mode='t'))\n",
    "print(dist_limited)\n",
    "pp.pprint(dist_limited.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dictionary = gensim.corpora.Dictionary(tokenize_corpus('./resources/aclImdb/all/', '*.txt'))\n",
    "data = [[dictionary.num_docs, dictionary.num_pos, len(dictionary.token2id)]]\n",
    "df = pd.DataFrame(data)\n",
    "df.columns=['Numero de reviews analizadas', 'Numero de tokens analizados', 'Numero de tokens únicos actuales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('limited.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando normalización y filtrado de vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_filter_most_frequent(dictionary, dist_limited)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "plurals = normalize_dictionary(dictionary)\n",
    "dictionary.save('limited.normalized.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus4.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus4.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_keep_n_frequent(dictionary, dist_limited)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus5.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus5.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "lda_model.save('limited.normalized.filtered.model')\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando Vocabulario Polarizado\n",
    "\n",
    "En todos los modelos anteriores hemos visto que existen varios tipos de palabras que concurrente aparecen con bastante frecuencia en varios tópicos, pero que aportan escaso valor a la hora de categorizar por temáticas. Algunos ejemplos de estos tipos de palabras son nombres propios y verbos que podemos filtrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_filter_neutral(dictionary, polarity=0.5):\n",
    "    from textblob import TextBlob\n",
    "    neutrals = []\n",
    "    for token in dictionary.values():\n",
    "        if len(token) > 1:\n",
    "            upper = token[0].upper() + token[1:]\n",
    "        blob = TextBlob(upper)\n",
    "        if abs(blob.polarity) <= polarity and blob.pos_tags[0][1] != 'NNP' and not blob.pos_tags[0][1].startswith('VB'):\n",
    "            neutrals.append(dictionary.token2id[token])\n",
    "                    \n",
    "    dictionary.filter_tokens(good_ids=neutrals)\n",
    "    dictionary.compactify()\n",
    "    return neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "print('Número de palabras iniciales: {}'.format(len(dictionary)))\n",
    "neutrals = dictionary_filter_neutral(dictionary, 0.0)\n",
    "print(\"Número de palabras neutrales: {}\".format(len(neutrals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "dictionary.save('final.dict')\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "gensim.corpora.MmCorpus.serialize('corpus6.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus6.mm')\n",
    "%time lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary)\n",
    "print_lda_model(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('neutral.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando otros Modelos de Representación: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('normalized.dict')\n",
    "dictionary_keep_n_frequent(dictionary, dist)\n",
    "corpus = MovieCorpus(\"./resources/aclImdb/all\", dictionary)\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "gensim.corpora.MmCorpus.serialize('corpus7.mm', corpus)\n",
    "corpus = gensim.corpora.MmCorpus('corpus7.mm')\n",
    "%time lda_model_tfidf = gensim.models.ldamodel.LdaModel(tfidf[corpus], num_topics=20, id2word=dictionary)\n",
    "pp.pprint(lda_model_tfidf.print_topics(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Nuestro Modelo como Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "r = requests.get(\"http://www.omdbapi.com/?i=tt0379889&apikey=ccedfaeb\")\n",
    "pp.pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando una Review Positiva\n",
    "\n",
    "Para analizar las reviews, primero vamos a tokenizar el texto y lo vamos a convertir en una representación Bag of Words con proyección a nuestro diccionario. Esta representación es la que podemos pasar a nuestro modelo LDA para que nos devuelva la distribución de tópicos más probable sobre nuestro texto inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('final.dict')\n",
    "good_review_text = \"\"\"I just saw this at the Toronto International Film Festival in the beautiful Elgin Theatre. \n",
    "I was blown away by the beautiful cinematography, the brilliant adaptation of a very tricky play and last \n",
    "but not least, the bravura performance of Al Pacino, who was born to play this role, \n",
    "which was perfectly balanced by an equally strong performance from Jeremy Irons.<br /><br />\n",
    "The film deftly explores the themes of love vs loyalty, law vs justice, and passion vs reason. \n",
    "Some might protest that the content is inherently anti-semitic, \n",
    "however they should consider the historical context of the story, \n",
    "and the delicate and nuanced way in which it is told in this adaptation\"\"\"\n",
    "good_review_tokens = analyzer(good_review_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(good_review_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuales son los 10 tokens más prominentes del tópico asignado con más probabilidad, el **tópico 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_tokens(model, topic_id, n_tokens=10):\n",
    "    terms = model.show_topic(topic_id, n_tokens)\n",
    "    return [item[0] for item in terms]\n",
    "tokens = get_topic_tokens(lda_model, 6, 20)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_tokens = list(set(good_review_tokens) & set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Sentimiento sobre los Keywords de los Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def explore_opinions(text, keywords):\n",
    "    from textblob import TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    data = []\n",
    "    for sentence in blob.sentences:\n",
    "        for token in keywords:\n",
    "            if token in sentence.words:\n",
    "                data.append([token, sentence.__str__(), sentence.sentiment[0], sentence.sentiment[1]])\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['Token', 'Sentence', 'Sentiment Polarity', 'Sentiment Subjectivity']\n",
    "    return df\n",
    "        \n",
    "display(HTML(explore_opinions(good_review_text, shared_tokens).to_html().replace(\"\\\\n\",\"<br>\").replace('adaptation', '<strong>adaptation</strong>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando una Review Negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review_text = \"\"\"I have to admit that although I'm a fan of Shakespeare, \n",
    "I was never really familiar with this play. And what I really can't say is whether this is a poor adaptation, \n",
    "or whether the play is just a bad choice for film. \n",
    "There are some nice pieces of business in it, but the execution is very clunky and the plot is obvious. \n",
    "The theme of the play is on the nature of debt, using the financial idea of debt and justice as a \n",
    "metaphor for emotional questions. That becomes clear when the issue of the rings becomes more important than \n",
    "the business with Shylock, which unfortunately descends into garden variety anti-Semitisim despite \n",
    "the Bard's best attempts to salvage him with a couple nice monologues.<br /><br />\n",
    "Outside of Jeremy Irons' dignified turn, I didn't think there was a decent performance in the bunch. \n",
    "Pacino's Yiddish consists of a slight whine added to the end of every pronouncement, and \n",
    "some of the better Shylock scenes are reduced to variations on the standard \"Pacino gets angry\" \n",
    "scene that his fans know and love. But Lynn Collins is outright embarrassing, to the point where I \n",
    "would have thought they would have screen-tested her right out of the picture early on. \n",
    "When she goes incognito as a man, it's hard not to laugh at all the things we're not supposed to laugh at. \n",
    "With Joseph Fiennes standing there trying to look sincere and complicated, it's hard not to make \n",
    "devastating comparisons to Gwyneth Paltrow's performance in \"Shakespeare in Love.\" \n",
    "The big problem however that over-rides everything in this film is just a lack of emotional focus. \n",
    "It's really hard to tell whether this film is trying to be a somewhat serious comedy or a strangely silly drama. \n",
    "Surely a good summer stock performance would wring more laughs from the material than this somber production. \n",
    "The actors seem embarrassed to be attempting humor, and unsure of where to place dramatic and comedic emphasis. \n",
    "All of this is basically the fault of the director, Michael Radford, who seems to think that the material \n",
    "is a great deal heavier than it appears to me.\"\"\"\n",
    "bad_review_tokens = analyzer(bad_review_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(bad_review_tokens))\n",
    "list(set(bad_review_tokens) & set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(explore_opinions(bad_review_text, shared_tokens).to_html().replace(\"\\\\n\",\"<br>\").replace('adaptation', '<strong>adaptation</strong>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando Reviews Nuevas fuera del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_text = \"\"\"Drug wars, meth, the lot. I thought no thank you. \n",
    "I kept hearing how good it was and I kept saying: \"No thank you\" \n",
    "Last January I got sick, one of those illnesses you can't quite figure out. \n",
    "Maybe it was pre and post election depression, I don't know. But I stayed in bed for almost \n",
    "10 days and then it happened. I saw the first episode and I was immediately and I mean immediately, \n",
    "hooked. I saw the entire series in 9 days. Voraciously. Now I had time to reflect. Why I wonder. \n",
    "When I think about it the first thing that comes to mind is not a thing it's Bryan Cranston. \n",
    "I know the concept was superb as was the writing but Bryan Cranston made it all real. \n",
    "His performance, the creation of Walter White will be studied in the Acting classes of the future. \n",
    "He is the one that pulls you forward - as well as backwards and sideways - then I realized that his \n",
    "creation acquired the power that it acquired, in great part thanks to the extraordinary cast of supporting players. \n",
    "I could write a page for each one of them but I'm just going to mention Aaron Paul. \n",
    "I ended up loving him. I developed a visceral need to see him find a way out. Well, what can I tell you. \n",
    "I know that one day, maybe when my kids are old enough, I shall see \"Breaking Bad\" again. I can't wait.\"\"\"\n",
    "bb_review_tokens = analyzer(bb_text)\n",
    "lda_model.get_document_topics(dictionary.doc2bow(bb_review_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topic(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_text_2 = \"\"\"What do you get when you have a chemistry teacher in a mid life crisis, dying of cancer, \n",
    "and washing cars as a second job to make ends meet for his middle class family? One of the greatest television \n",
    "dramas of all time with crazy plot twists, brilliant performances, and unforgettable characters and cinematography.\n",
    "There is so much to like about the masterpiece that is Breaking Bad. Take your pick: the acting, \n",
    "the writing, the story lines, the plot, the suspense the cliff hangers, the action scenes, the camera work, \n",
    "the characters, the character arcs, the realism, the satirical style, any season, the end, the casting, the \n",
    "dark humor and humor relief, the scenery, the contrast between background and foreground to establish artistic \n",
    "effect (the sun shiny clear blue skies of the NM desert behind the gruesome organized crime and violence of the \n",
    "underworld), the mixing of favorite genres (crime caper, dark comedy, western, noir, horror, suspense, action, \n",
    "drama, thriller, Shakespearean tragedy, dystopia, psychological character study..), the lines/quotes...\n",
    "the list goes on.\n",
    "What's amazing about Breaking Bad is it begins so humble and quiet, and as it continues to let its' story unfold,\n",
    "it explodes. It gets better and better each season until the end in the final season, we don't know if we're watching a\n",
    "television show or an Academy Award winning motion picture. The show dares to go where no one would have thought \n",
    "it would go- into a transcendent realm of classic cinema- and it pulls it off beautifully.\"\"\"\n",
    "bb_review_tokens = analyzer(bb_text_2)\n",
    "lda_model.get_document_topics(lda_model.id2word.doc2bow(bb_review_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 2))))\n",
    "lda_model.show_topic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 4))))\n",
    "lda_model.show_topic(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(bb_review_tokens) & set(get_topic_tokens(lda_model, 18))))\n",
    "lda_model.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
